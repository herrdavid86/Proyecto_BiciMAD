{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97a9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas\n",
    "#! pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944722ba",
   "metadata": {},
   "source": [
    "### Este código genera un solo dataset de los dos casos json y csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb55f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando la ejecución principal del procesador de datos unificado y optimizado.\n",
      "\n",
      "--- Columnas del archivo de estaciones después de cargar y limpiar espacios: ---\n",
      "['id', 'Número', 'Gis_X', 'Gis_Y', 'Fecha de Alta', 'Distrito', 'Barrio', 'Calle', 'Nº Finca', 'Tipo de Reserva', 'Número de Plazas', 'Longitud', 'Latitud', 'Direccion', 'E. Temp. Código 1', 'E. Temp. Código 2', 'E. Temp. Código 3', 'E. Prec. Código 1', 'E. Prec. Código 2']\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "Archivo de estaciones 'estaciones_bicimad_meteo_15_05.csv' cargado exitosamente. 271 estaciones disponibles para referencia.\n",
      "\n",
      "--- INICIO DEL PROCESAMIENTO GENERAL ---\n",
      "Archivos a procesar (en este orden): 12\n",
      "\n",
      "--- Procesando archivo: 201801_Usage_Bicimad.json (1/12) ---\n",
      "  Archivo JSON '201801_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201801_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201802_Usage_Bicimad.json (2/12) ---\n",
      "  Archivo JSON '201802_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201802_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201803_Usage_Bicimad.json (3/12) ---\n",
      "  Archivo JSON '201803_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201803_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201804_Usage_Bicimad.json (4/12) ---\n",
      "  Archivo JSON '201804_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201804_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201805_Usage_Bicimad.json (5/12) ---\n",
      "  Archivo JSON '201805_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201805_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201806_Usage_Bicimad.json (6/12) ---\n",
      "  Archivo JSON '201806_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201806_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201807_Usage_Bicimad.json (7/12) ---\n",
      "  Archivo JSON '201807_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201807_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201808_Usage_Bicimad.json (8/12) ---\n",
      "  Archivo JSON '201808_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201808_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201809_Usage_Bicimad.json (9/12) ---\n",
      "  Archivo JSON '201809_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201809_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201810_Usage_Bicimad.json (10/12) ---\n",
      "  Archivo JSON '201810_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201810_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201811_Usage_Bicimad.json (11/12) ---\n",
      "  Archivo JSON '201811_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201811_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Procesando archivo: 201812_Usage_Bicimad.json (12/12) ---\n",
      "  Archivo JSON '201812_Usage_Bicimad.json' cargado.\n",
      "  Archivo '201812_Usage_Bicimad.json' procesado y añadido a la lista para consolidación.\n",
      "\n",
      "--- Realizando la concatenación de todos los DataFrames procesados ---\n",
      "Concatenación completada. Filas totales: 3678086\n",
      "\n",
      "--- Realizando conversiones finales a tipos enteros donde sea posible (optimizado) ---\n",
      "  Intentando convertir columna 'travel_time' (Dtype actual: int64)...\n",
      "  Columna 'travel_time' convertida a Int64.\n",
      "  Intentando convertir columna 'idplug_base' (Dtype actual: int64)...\n",
      "  Columna 'idplug_base' convertida a Int64.\n",
      "  Intentando convertir columna 'idplug_station' (Dtype actual: int64)...\n",
      "  Columna 'idplug_station' convertida a Int64.\n",
      "  Intentando convertir columna 'idunplug_base' (Dtype actual: int64)...\n",
      "  Columna 'idunplug_base' convertida a Int64.\n",
      "  Intentando convertir columna 'idunplug_station' (Dtype actual: int64)...\n",
      "  Columna 'idunplug_station' convertida a Int64.\n",
      "Conversiones a enteros completadas.\n",
      "\n",
      "--- Formateando 'unplug_hourTime' a 'YYYY-MM-DDTHH:MM:SS' (string) ---\n",
      "Formato final aplicado a 'unplug_hourTime'.\n",
      "\n",
      "--- FIN DEL PROCESAMIENTO GENERAL ---\n",
      "\n",
      "--- RESULTADO FINAL: DataFrame Consolidado ---\n",
      "Primeras 5 filas:\n",
      "       unplug_hourTime  travel_time  idunplug_station  idplug_station  \\\n",
      "0  2018-01-01T00:00:00          284                 6               7   \n",
      "1  2018-01-01T00:00:00          666                24             117   \n",
      "2  2018-01-01T00:00:00          662                24             117   \n",
      "3  2018-01-01T00:00:00          708                82             110   \n",
      "4  2018-01-01T00:00:00          171               169              58   \n",
      "\n",
      "   idplug_base  idunplug_base unlock_station_name lock_station_name  \n",
      "0            1             14                 NaN               NaN  \n",
      "1            4             21                 NaN               NaN  \n",
      "2           19             19                 NaN               NaN  \n",
      "3            1              2                 NaN               NaN  \n",
      "4            5              3                 NaN               NaN  \n",
      "\n",
      "Dimensiones del DataFrame consolidado: (3678086, 8)\n",
      "\n",
      "Tipos de datos del DataFrame consolidado (después de conversiones finales):\n",
      "unplug_hourTime        object\n",
      "travel_time             Int64\n",
      "idunplug_station        Int64\n",
      "idplug_station          Int64\n",
      "idplug_base             Int64\n",
      "idunplug_base           Int64\n",
      "unlock_station_name    object\n",
      "lock_station_name      object\n",
      "dtype: object\n",
      "\n",
      "Columnas del DataFrame consolidado:\n",
      "['unplug_hourTime', 'travel_time', 'idunplug_station', 'idplug_station', 'idplug_base', 'idunplug_base', 'unlock_station_name', 'lock_station_name']\n",
      "\n",
      "La columna 'unplug_hourTime' ahora es de tipo: object\n",
      "\n",
      "¡Excelente! No se encontraron valores nulos/inválidos en 'unplug_hourTime' en el DataFrame final.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from math import sqrt\n",
    "import random\n",
    "import re \n",
    "\n",
    "# --- CONFIGURACIÓN DE RUTAS ---\n",
    "main_data_folder_path = 'datos/Usos bicimad/bicimad_2018/Consolidado' # Ruta de la carpeta con JSON/CSV de viajes\n",
    "#C:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\datos\\Usos bicimad\\bicimad_2018\n",
    "stations_folder_path = 'datos' # Ruta de la carpeta con el archivo de estaciones\n",
    "stations_file_name = 'estaciones_bicimad_meteo_15_05.csv' # Usando el nombre de archivo que te funcionó\n",
    "stations_file_path = os.path.join(stations_folder_path, stations_file_name)\n",
    "\n",
    "\n",
    "# DEFINIR LA LISTA DE ARCHIVOS A PROCESAR Y SU ORDEN\n",
    "files_to_process = [f for f in os.listdir(main_data_folder_path) if f.endswith('.json') or f.endswith('.csv')]\n",
    "\n",
    "print(\"Iniciando la ejecución principal del procesador de datos unificado y optimizado.\")\n",
    "\n",
    "# --- CARGAR Y PREPROCESAR ARCHIVO DE ESTACIONES ---\n",
    "df_estaciones = pd.DataFrame()\n",
    "estaciones_lookup = {}\n",
    "if os.path.exists(stations_file_path):\n",
    "    try:\n",
    "        df_estaciones = pd.read_csv(stations_file_path, encoding='utf-8', sep=';')\n",
    "        df_estaciones.columns = df_estaciones.columns.str.strip() \n",
    "        \n",
    "        print(\"\\n--- Columnas del archivo de estaciones después de cargar y limpiar espacios: ---\")\n",
    "        print(df_estaciones.columns.tolist())\n",
    "        print(\"-----------------------------------------------------------------------\\n\")\n",
    "\n",
    "        station_column_mapping = {\n",
    "            'id': 'station_id',\n",
    "            'Longitud': 'longitude',\n",
    "            'Latitud': 'latitude',\n",
    "            'Número de Plazas': 'num_docks'\n",
    "        }\n",
    "        \n",
    "        existing_cols_to_rename = {k: v for k, v in station_column_mapping.items() if k in df_estaciones.columns}\n",
    "        df_estaciones = df_estaciones.rename(columns=existing_cols_to_rename)\n",
    "        \n",
    "        required_station_cols = ['station_id', 'longitude', 'latitude', 'num_docks']\n",
    "        if not all(col in df_estaciones.columns for col in required_station_cols):\n",
    "            missing_cols = [col for col in required_station_cols if col not in df_estaciones.columns]\n",
    "            raise ValueError(f\"Faltan columnas requeridas en el archivo de estaciones después de renombrar: {missing_cols}. Revise el mapeo de columnas y los nombres originales.\")\n",
    "\n",
    "        df_estaciones['longitude'] = pd.to_numeric(df_estaciones['longitude'], errors='coerce')\n",
    "        df_estaciones['latitude'] = pd.to_numeric(df_estaciones['latitude'], errors='coerce')\n",
    "        df_estaciones['station_id'] = pd.to_numeric(df_estaciones['station_id'], errors='coerce')\n",
    "        df_estaciones['num_docks'] = pd.to_numeric(df_estaciones['num_docks'], errors='coerce')\n",
    "\n",
    "        rows_before_dropna = len(df_estaciones)\n",
    "        df_estaciones.dropna(subset=['station_id', 'longitude', 'latitude', 'num_docks'], inplace=True)\n",
    "        rows_after_dropna = len(df_estaciones)\n",
    "\n",
    "        if rows_before_dropna > rows_after_dropna:\n",
    "            print(f\"  Se eliminaron {rows_before_dropna - rows_after_dropna} filas del archivo de estaciones debido a valores nulos en columnas críticas.\")\n",
    "        \n",
    "        if not df_estaciones.empty:\n",
    "            df_estaciones['station_id'] = pd.to_numeric(df_estaciones['station_id'], errors='coerce').astype(pd.Int64Dtype())\n",
    "            estaciones_lookup = df_estaciones.set_index('station_id')[['latitude', 'longitude', 'num_docks']].to_dict('index')\n",
    "            print(f\"Archivo de estaciones '{stations_file_name}' cargado exitosamente. {len(df_estaciones)} estaciones disponibles para referencia.\")\n",
    "        else:\n",
    "            print(f\"ADVERTENCIA: El archivo de estaciones '{stations_file_name}' terminó vacío después del preprocesamiento. 0 estaciones disponibles para referencia.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: No se pudo cargar o preprocesar el archivo de estaciones '{stations_file_path}': {e}\")\n",
    "        df_estaciones = pd.DataFrame()\n",
    "else:\n",
    "    print(f\"ADVERTENCIA: Archivo de estaciones no encontrado en '{stations_file_path}'. No se rellenarán IDs de estación faltantes.\")\n",
    "\n",
    "# Función para calcular la distancia euclidiana (aproximación para distancias cortas)\n",
    "def euclidean_distance(lat1, lon1, lat2, lon2):\n",
    "    return sqrt((lat2 - lat1)**2 + (lon2 - lon1)**2)\n",
    "\n",
    "# Función para encontrar la estación más cercana\n",
    "def find_nearest_station_id(target_lat, target_lon, stations_dict):\n",
    "    if pd.isna(target_lat) or pd.isna(target_lon):\n",
    "        return pd.NA, pd.NA\n",
    "\n",
    "    min_dist = float('inf')\n",
    "    nearest_station_id = pd.NA\n",
    "    nearest_num_docks = pd.NA\n",
    "\n",
    "    for station_id, data in stations_dict.items():\n",
    "        s_lat = data['latitude']\n",
    "        s_lon = data['longitude']\n",
    "        s_num_docks = data['num_docks']\n",
    "\n",
    "        if pd.isna(s_lat) or pd.isna(s_lon):\n",
    "            continue\n",
    "\n",
    "        dist = euclidean_distance(target_lat, target_lon, s_lat, s_lon)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            nearest_station_id = station_id\n",
    "            nearest_num_docks = s_num_docks\n",
    "    return nearest_station_id, nearest_num_docks\n",
    "\n",
    "\n",
    "# Inicializar el DataFrame consolidado\n",
    "all_processed_dfs = []\n",
    "\n",
    "# --- MODIFICACIÓN: Columnas finales esperadas. Eliminadas 'ageRange' y 'user_day_code' ---\n",
    "final_target_columns = [\n",
    "    'unplug_hourTime', 'travel_time', 'idunplug_station', 'idplug_station',\n",
    "    'idplug_base', 'idunplug_base',\n",
    "    'unlock_station_name', 'lock_station_name'\n",
    "]\n",
    "\n",
    "print(f\"\\n--- INICIO DEL PROCESAMIENTO GENERAL ---\")\n",
    "print(f\"Archivos a procesar (en este orden): {len(files_to_process)}\")\n",
    "\n",
    "for i, file in enumerate(files_to_process):\n",
    "    file_path = os.path.join(main_data_folder_path, file)\n",
    "    print(f\"\\n--- Procesando archivo: {file} ({i + 1}/{len(files_to_process)}) ---\")\n",
    "\n",
    "    df_temp = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        if file.endswith('.json'):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    data = [json.loads(line) for line in f]\n",
    "            except UnicodeDecodeError:\n",
    "                with open(file_path, 'r', encoding='ISO-8859-1', errors='replace') as f:\n",
    "                    data = [json.loads(line) for line in f]\n",
    "            \n",
    "            df_temp = pd.json_normalize(data)\n",
    "            print(f\"  Archivo JSON '{file}' cargado.\")\n",
    "            \n",
    "            df_temp.columns = df_temp.columns.str.strip()\n",
    "\n",
    "            # --- Procesamiento de unplug_hourTime para JSON ---\n",
    "            col_time_unplug = 'unplug_hourTime'\n",
    "            json_date_col_unplug = f'{col_time_unplug}.$date'\n",
    "\n",
    "            if json_date_col_unplug in df_temp.columns:\n",
    "                df_temp[col_time_unplug] = df_temp[json_date_col_unplug].apply(lambda x: x.get('$date') if isinstance(x, dict) else x)\n",
    "                df_temp = df_temp.drop(columns=[json_date_col_unplug])\n",
    "            elif col_time_unplug not in df_temp.columns:\n",
    "                df_temp[col_time_unplug] = pd.NA \n",
    "            \n",
    "            # Convertir unplug_hourTime a string para aplicar regex si es necesario\n",
    "            df_temp[col_time_unplug] = df_temp[col_time_unplug].astype(str)\n",
    "\n",
    "            # Limpiar la 'Z' y los offsets como +0100 o -0200, y milisegundos.\n",
    "            # Esto convierte \"2019-07-01T00:00:00Z\" o \"2019-07-01T00:00:00.123+0100\" a \"2019-07-01T00:00:00\"\n",
    "            df_temp[col_time_unplug] = df_temp[col_time_unplug].str.replace(r'\\.\\d+', '', regex=True) # Elimina milisegundos\n",
    "            df_temp[col_time_unplug] = df_temp[col_time_unplug].str.replace(r'[Z]$', '', regex=True) # Elimina 'Z' al final\n",
    "            df_temp[col_time_unplug] = df_temp[col_time_unplug].str.replace(r'[+-]\\d{4}$', '', regex=True) # Elimina offsets como +0100\n",
    "\n",
    "            # Convertir a datetime inmediatamente para liberar memoria antes de concatenar\n",
    "            df_temp[col_time_unplug] = pd.to_datetime(df_temp[col_time_unplug], errors='coerce')\n",
    "\n",
    "\n",
    "            # Extracción de coordenadas JSON (sin cambios)\n",
    "            for prefix, lat_col, lon_col in [\n",
    "                ('unplug_station.location.coordinates', 'temp_lat_unlock', 'temp_lon_unlock'),\n",
    "                ('plug_station.location.coordinates', 'temp_lat_lock', 'temp_lon_lock'),\n",
    "            ]:\n",
    "                if prefix in df_temp.columns:\n",
    "                    df_temp[lon_col] = df_temp[prefix].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 2 else pd.NA)\n",
    "                    df_temp[lat_col] = df_temp[prefix].apply(lambda x: x[1] if isinstance(x, list) and len(x) == 2 else pd.NA)\n",
    "                else:\n",
    "                    if lat_col not in df_temp.columns: df_temp[lat_col] = pd.NA\n",
    "                    if lon_col not in df_temp.columns: df_temp[lon_col] = pd.NA\n",
    "\n",
    "            # IDs de estación JSON (sin cambios)\n",
    "            if 'idunplug_station' not in df_temp.columns and 'unplug_station.id' in df_temp.columns:\n",
    "                 df_temp['idunplug_station'] = pd.to_numeric(df_temp['unplug_station.id'], errors='coerce') \n",
    "            if 'idplug_station' not in df_temp.columns and 'plug_station.id' in df_temp.columns:\n",
    "                 df_temp['idplug_station'] = pd.to_numeric(df_temp['plug_station.id'], errors='coerce')\n",
    "\n",
    "            # Travel time JSON (sin cambios)\n",
    "            if 'travel_time' in df_temp.columns:\n",
    "                df_temp['travel_time'] = pd.to_numeric(df_temp['travel_time'], errors='coerce')\n",
    "            \n",
    "            # Columnas a eliminar específicas de JSON y las que quieres eliminar\n",
    "            cols_to_drop_json_specific = [\n",
    "                '_id.$oid', '_id', \n",
    "                'unplug_station.location.coordinates', 'plug_station.location.coordinates',\n",
    "                'unplug_station.location.type', 'plug_station.location.type',\n",
    "                'unplug_station.address', 'plug_station.address', 'track.features', 'track.type',\n",
    "                'unplug_station.id', 'plug_station.id',\n",
    "                'lock_date.$date' # Eliminada explícitamente si existe\n",
    "            ]\n",
    "            \n",
    "            # ELIMINAR user_type, zip_code, _id_oid y lock_date si existen\n",
    "            # --- MODIFICACIÓN: Eliminadas 'ageRange' y 'user_day_code' de esta lista para el JSON ---\n",
    "            for col in ['_id_oid', 'user_type', 'zip_code', 'lock_date', 'ageRange', 'user_day_code']: \n",
    "                if col in df_temp.columns:\n",
    "                    df_temp = df_temp.drop(columns=[col])\n",
    "\n",
    "            cols_to_drop_json_from_df = [col for col in cols_to_drop_json_specific if col in df_temp.columns]\n",
    "            if cols_to_drop_json_from_df:\n",
    "                df_temp = df_temp.drop(columns=cols_to_drop_json_from_df)\n",
    "            \n",
    "            # Asegurar existencia de columnas de nombre de estación\n",
    "            for col in ['unlock_station_name', 'lock_station_name']:\n",
    "                if col not in df_temp.columns:\n",
    "                    df_temp[col] = pd.NA\n",
    "\n",
    "\n",
    "        elif file.endswith('.csv'):\n",
    "            # --- LÓGICA DE PROCESAMIENTO CSV ---\n",
    "            df_temp = pd.read_csv(file_path, encoding='ISO-8859-1', sep=';', on_bad_lines='skip', skip_blank_lines=True) \n",
    "            print(f\"  Archivo CSV '{file}' cargado.\")\n",
    "\n",
    "            df_temp.columns = df_temp.columns.str.strip()\n",
    "            df_temp.replace('', pd.NA, inplace=True)\n",
    "\n",
    "            # Mapeo de columnas de CSV a nombres unificados\n",
    "            csv_column_mapping = {\n",
    "                'trip_minutes': 'travel_time',\n",
    "                'unlock_date': 'unplug_hourTime', \n",
    "                'station_unlock': 'idunplug_station', 'dock_unlock': 'idunplug_base',      \n",
    "                'station_lock': 'idplug_station', 'dock_lock': 'idplug_base',          \n",
    "                'lock_date': 'lock_date_raw', # Leemos lock_date con un nombre temporal para el fallback\n",
    "                'unlock_station_name': 'unlock_station_name',\n",
    "                'lock_station_name': 'lock_station_name',\n",
    "\n",
    "            }\n",
    "            existing_csv_cols_to_rename = {k: v for k, v in csv_column_mapping.items() if k in df_temp.columns}\n",
    "            df_temp = df_temp.rename(columns=existing_csv_cols_to_rename)\n",
    "            \n",
    "            # Convertir travel_time a numérico y a segundos (ya venía en minutos)\n",
    "            if 'travel_time' in df_temp.columns:\n",
    "                df_temp['travel_time'] = pd.to_numeric(df_temp['travel_time'], errors='coerce')\n",
    "                df_temp.loc[df_temp['travel_time'].notna(), 'travel_time'] = df_temp.loc[df_temp['travel_time'].notna(), 'travel_time'] * 60\n",
    "            else:\n",
    "                df_temp['travel_time'] = pd.NA\n",
    "\n",
    "            # --- Procesamiento de unplug_hourTime y lock_date para CSV ---\n",
    "            # Asegurarse de que ambas columnas existen para la lógica de fallback\n",
    "            if 'unplug_hourTime' not in df_temp.columns:\n",
    "                df_temp['unplug_hourTime'] = pd.NaT # Inicializar con NaT para fechas\n",
    "            if 'lock_date_raw' not in df_temp.columns:\n",
    "                df_temp['lock_date_raw'] = pd.NaT\n",
    "\n",
    "            # Primero, intentar convertir unplug_hourTime directamente\n",
    "            df_temp['unplug_hourTime'] = pd.to_datetime(df_temp['unplug_hourTime'], errors='coerce')\n",
    "            \n",
    "            # Convertir lock_date_raw a datetime para el fallback\n",
    "            df_temp['lock_date_raw'] = pd.to_datetime(df_temp['lock_date_raw'], errors='coerce')\n",
    "\n",
    "            # Lógica de FALLBACK: Si unplug_hourTime es NaT, intentar calcularlo\n",
    "            # desde lock_date_raw y travel_time\n",
    "            mask_unplug_nan = df_temp['unplug_hourTime'].isna()\n",
    "            mask_lock_valid = df_temp['lock_date_raw'].notna()\n",
    "            mask_travel_valid = df_temp['travel_time'].notna() & (df_temp['travel_time'] >= 0) # Asegurar travel_time no negativo\n",
    "            \n",
    "            # Aplicar fallback solo a las filas que necesitan y tienen datos válidos\n",
    "            fallback_mask = mask_unplug_nan & mask_lock_valid & mask_travel_valid\n",
    "            \n",
    "            if fallback_mask.any():\n",
    "                print(f\"  Aplicando lógica de fallback para {fallback_mask.sum()} filas con 'unplug_hourTime' nulo en '{file}'.\")\n",
    "                df_temp.loc[fallback_mask, 'unplug_hourTime'] = \\\n",
    "                    df_temp.loc[fallback_mask, 'lock_date_raw'] - \\\n",
    "                    pd.to_timedelta(df_temp.loc[fallback_mask, 'travel_time'], unit='s')\n",
    "            \n",
    "            # Eliminar la columna temporal lock_date_raw ya que ya no la necesitamos\n",
    "            if 'lock_date_raw' in df_temp.columns:\n",
    "                df_temp = df_temp.drop(columns=['lock_date_raw'])\n",
    "\n",
    "\n",
    "            # --- LÓGICA PARA EXTRACCIÓN CONDICIONAL DE COORDENADAS CSV TEMPORALES ---\n",
    "            def extract_coords_from_csv_geo_safe(geo_str_val):\n",
    "                if pd.isna(geo_str_val) or not isinstance(geo_str_val, str):\n",
    "                    return pd.NA, pd.NA\n",
    "                try:\n",
    "                    if geo_str_val.strip().startswith('(') and geo_str_val.strip().endswith(')'):\n",
    "                        coords_clean = geo_str_val.strip('()').split(',')\n",
    "                        if len(coords_clean) == 2:\n",
    "                            lat = float(coords_clean[0].strip().replace(',', '.'))\n",
    "                            lon = float(coords_clean[1].strip().replace(',', '.'))\n",
    "                            return lat, lon\n",
    "                    else:\n",
    "                        json_str = geo_str_val.replace(\"'\", '\"')\n",
    "                        geo_data = json.loads(json_str)\n",
    "                        if 'coordinates' in geo_data and isinstance(geo_data['coordinates'], list) and len(geo_data['coordinates']) == 2:\n",
    "                            lat = float(str(geo_data['coordinates'][1]).replace(',', '.'))\n",
    "                            lon = float(str(geo_data['coordinates'][0]).replace(',', '.'))\n",
    "                            return lat, lon\n",
    "                except (json.JSONDecodeError, ValueError, TypeError, IndexError):\n",
    "                    pass\n",
    "                return pd.NA, pd.NA\n",
    "            \n",
    "            if 'temp_lat_unlock' not in df_temp.columns: df_temp['temp_lat_unlock'] = pd.NA\n",
    "            if 'temp_lon_unlock' not in df_temp.columns: df_temp['temp_lon_unlock'] = pd.NA\n",
    "            if 'temp_lat_lock' not in df_temp.columns: df_temp['temp_lat_lock'] = pd.NA\n",
    "            if 'temp_lon_lock' not in df_temp.columns: df_temp['temp_lon_lock'] = pd.NA\n",
    "\n",
    "            if 'geolocation_unlock' in df_temp.columns:\n",
    "                df_temp['idunplug_station'] = pd.to_numeric(df_temp['idunplug_station'], errors='coerce') \n",
    "                mask_unlock_id_nan = df_temp['idunplug_station'].isna()\n",
    "                if mask_unlock_id_nan.any():\n",
    "                    df_temp.loc[mask_unlock_id_nan, ['temp_lat_unlock', 'temp_lon_unlock']] = \\\n",
    "                        df_temp.loc[mask_unlock_id_nan, 'geolocation_unlock'].apply(\n",
    "                            lambda x: pd.Series(extract_coords_from_csv_geo_safe(x), dtype='object')).values\n",
    "            \n",
    "            if 'geolocation_lock' in df_temp.columns:\n",
    "                df_temp['idplug_station'] = pd.to_numeric(df_temp['idplug_station'], errors='coerce') \n",
    "                mask_lock_id_nan = df_temp['idplug_station'].isna()\n",
    "                if mask_lock_id_nan.any():\n",
    "                    df_temp.loc[mask_lock_id_nan, ['temp_lat_lock', 'temp_lon_lock']] = \\\n",
    "                        df_temp.loc[mask_lock_id_nan, 'geolocation_lock'].apply(\n",
    "                            lambda x: pd.Series(extract_coords_from_csv_geo_safe(x), dtype='object')).values\n",
    "            \n",
    "            # Eliminar filas completamente vacías (sin cambios)\n",
    "            initial_rows = len(df_temp)\n",
    "            df_temp.dropna(how='all', inplace=True)\n",
    "            rows_dropped = initial_rows - len(df_temp)\n",
    "            if rows_dropped > 0:\n",
    "                print(f\"  Se eliminaron {rows_dropped} filas completamente vacías de '{file}'.\")\n",
    "\n",
    "            # Columnas a eliminar específicas de CSV\n",
    "            csv_columns_to_exclude_specific = {\n",
    "                'fecha', 'idBike', 'fleet', 'address_unlock', 'locktype',\n",
    "                'unlocktype', 'address_lock', 'geolocation',\n",
    "                'geolocation_unlock', 'geolocation_lock'\n",
    "            }\n",
    "            \n",
    "            # ELIMINAR user_type, zip_code, _id_oid si existen\n",
    "            # --- MODIFICACIÓN: Eliminadas 'ageRange' y 'user_day_code' de esta lista para el CSV ---\n",
    "            for col in ['_id_oid', 'user_type', 'zip_code', 'ageRange', 'user_day_code']: \n",
    "                if col in df_temp.columns:\n",
    "                    df_temp = df_temp.drop(columns=[col])\n",
    "\n",
    "            cols_to_drop_from_temp = [col for col in csv_columns_to_exclude_specific if col in df_temp.columns]\n",
    "            if cols_to_drop_from_temp:\n",
    "                df_temp = df_temp.drop(columns=cols_to_drop_from_temp)\n",
    "            \n",
    "            # Asegurar existencia de columnas de nombre de estación\n",
    "            for col in ['unlock_station_name', 'lock_station_name']:\n",
    "                if col not in df_temp.columns:\n",
    "                    df_temp[col] = pd.NA\n",
    "\n",
    "        else:\n",
    "            print(f\"  ADVERTENCIA: Archivo no reconocido (no .json ni .csv): '{file}'. Saltando.\")\n",
    "            continue\n",
    "\n",
    "        # --- LÓGICA DE RELLENO DE IDS DE ESTACIÓN Y BASE ---\n",
    "        if estaciones_lookup:\n",
    "            # Relleno para unplug_station\n",
    "            if 'idunplug_station' in df_temp.columns:\n",
    "                df_temp['idunplug_station'] = pd.to_numeric(df_temp['idunplug_station'], errors='coerce')\n",
    "                unplug_mask_nan = df_temp['idunplug_station'].isna()\n",
    "                if unplug_mask_nan.any():\n",
    "                    temp_coords_for_lookup = df_temp.loc[unplug_mask_nan, ['temp_lat_unlock', 'temp_lon_unlock']]\n",
    "                    if not temp_coords_for_lookup.empty:\n",
    "                        results_unplug = temp_coords_for_lookup.apply(\n",
    "                            lambda row: pd.Series(find_nearest_station_id(row['temp_lat_unlock'], row['temp_lon_unlock'], estaciones_lookup), dtype='object'), \n",
    "                            axis=1\n",
    "                        )\n",
    "                        df_temp.loc[unplug_mask_nan, 'idunplug_station'] = results_unplug[0]\n",
    "                        \n",
    "                        base_unplug_mask_nan = df_temp.loc[unplug_mask_nan, 'idunplug_base'].isna() & df_temp.loc[unplug_mask_nan, 'idunplug_station'].notna()\n",
    "                        if base_unplug_mask_nan.any():\n",
    "                            docks_for_random = results_unplug[1].loc[base_unplug_mask_nan.index] \n",
    "                            valid_docks_mask = docks_for_random.notna() & (docks_for_random > 0)\n",
    "                            if valid_docks_mask.any():\n",
    "                                df_temp.loc[base_unplug_mask_nan.index[valid_docks_mask], 'idunplug_base'] = \\\n",
    "                                    [random.randint(1, int(d)) for d in docks_for_random[valid_docks_mask]]\n",
    "\n",
    "            # Relleno para plug_station\n",
    "            if 'idplug_station' in df_temp.columns:\n",
    "                df_temp['idplug_station'] = pd.to_numeric(df_temp['idplug_station'], errors='coerce')\n",
    "                plug_mask_nan = df_temp['idplug_station'].isna()\n",
    "                if plug_mask_nan.any():\n",
    "                    temp_coords_for_lookup = df_temp.loc[plug_mask_nan, ['temp_lat_lock', 'temp_lon_lock']]\n",
    "                    if not temp_coords_for_lookup.empty:\n",
    "                        results_plug = temp_coords_for_lookup.apply(\n",
    "                            lambda row: pd.Series(find_nearest_station_id(row['temp_lat_lock'], row['temp_lon_lock'], estaciones_lookup), dtype='object'), \n",
    "                            axis=1\n",
    "                        )\n",
    "                        df_temp.loc[plug_mask_nan, 'idplug_station'] = results_plug[0]\n",
    "\n",
    "                        base_plug_mask_nan = df_temp.loc[plug_mask_nan, 'idplug_base'].isna() & df_temp.loc[plug_mask_nan, 'idplug_station'].notna()\n",
    "                        if base_plug_mask_nan.any():\n",
    "                            docks_for_random = results_plug[1].loc[base_plug_mask_nan.index]\n",
    "                            valid_docks_mask = docks_for_random.notna() & (docks_for_random > 0)\n",
    "                            if valid_docks_mask.any():\n",
    "                                df_temp.loc[base_plug_mask_nan.index[valid_docks_mask], 'idplug_base'] = \\\n",
    "                                    [random.randint(1, int(d)) for d in docks_for_random[valid_docks_mask]]\n",
    "\n",
    "\n",
    "        # Eliminar las columnas de coordenadas temporales usadas para el relleno (sin cambios)\n",
    "        cols_to_drop_temp_coords = ['temp_lat_unlock', 'temp_lon_unlock', 'temp_lat_lock', 'temp_lon_lock']\n",
    "        cols_to_drop_from_temp_df = [col for col in cols_to_drop_temp_coords if col in df_temp.columns]\n",
    "        if cols_to_drop_from_temp_df:\n",
    "            df_temp = df_temp.drop(columns=cols_to_drop_from_temp_df)\n",
    "        \n",
    "        # Asegurarse de que el DataFrame temporal tenga todas las columnas objetivo antes de añadirlo\n",
    "        # Las columnas que no existen en el df_temp pero están en final_target_columns serán rellenadas con NaN\n",
    "        df_temp = df_temp.reindex(columns=final_target_columns)\n",
    "        all_processed_dfs.append(df_temp)\n",
    "        print(f\"  Archivo '{file}' procesado y añadido a la lista para consolidación.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ERROR: Archivo no encontrado: '{file_path}'. Saltando este archivo.\")\n",
    "    except (json.JSONDecodeError, UnicodeDecodeError, ValueError) as e:\n",
    "        print(f\"  ERROR: Problema de codificación o formato en '{file_path}': {e}. Saltando este archivo.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"  ADVERTENCIA: Archivo vacío o con solo encabezados: '{file_path}'. Saltando este archivo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR inesperado al procesar '{file}': {e}. Saltando este archivo.\")\n",
    "\n",
    "# --- CONSOLIDACIÓN FINAL ---\n",
    "consolidated_df = pd.DataFrame()\n",
    "if all_processed_dfs:\n",
    "    print(\"\\n--- Realizando la concatenación de todos los DataFrames procesados ---\")\n",
    "    consolidated_df = pd.concat(all_processed_dfs, ignore_index=True)\n",
    "    print(f\"Concatenación completada. Filas totales: {len(consolidated_df)}\")\n",
    "else:\n",
    "    print(\"\\nNo se procesó ningún archivo exitosamente. El DataFrame consolidado estará vacío.\")\n",
    "\n",
    "# --- CONVERSIÓN FINAL A ENTEROS (OPTIMIZADA) ---\n",
    "if not consolidated_df.empty:\n",
    "    print(\"\\n--- Realizando conversiones finales a tipos enteros donde sea posible (optimizado) ---\")\n",
    "    # 'ageRange' ya no está en la lista de columnas a convertir\n",
    "    integer_cols_final = ['travel_time', 'idplug_base', 'idplug_station', 'idunplug_base', 'idunplug_station'] \n",
    "    for col in integer_cols_final:\n",
    "        if col in consolidated_df.columns:\n",
    "            print(f\"  Intentando convertir columna '{col}' (Dtype actual: {consolidated_df[col].dtype})...\")\n",
    "            \n",
    "            current_series = pd.to_numeric(consolidated_df[col], errors='coerce')\n",
    "            \n",
    "            if pd.api.types.is_float_dtype(current_series):\n",
    "                inf_mask = np.isinf(current_series)\n",
    "                if inf_mask.any():\n",
    "                    num_inf = inf_mask.sum()\n",
    "                    current_series.loc[inf_mask] = np.nan\n",
    "                    print(f\"    Se reemplazaron {num_inf} valores infinitos por NaN en '{col}'.\")\n",
    "            \n",
    "            try:\n",
    "                temp_float_series = current_series.astype(float)\n",
    "                # Solo redondear si hay parte decimal y no es NaN\n",
    "                if (temp_float_series.notna() & (temp_float_series != temp_float_series.astype(int))).any():\n",
    "                    temp_float_series = np.round(temp_float_series)\n",
    "\n",
    "                consolidated_df[col] = temp_float_series.astype(pd.Int64Dtype())\n",
    "                print(f\"  Columna '{col}' convertida a {consolidated_df[col].dtype}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ADVERTENCIA: No se pudo convertir '{col}' a Int64Dtype. Manteniendo el tipo numérico más general. Error: {e}\")\n",
    "                consolidated_df[col] = current_series \n",
    "                \n",
    "    print(\"Conversiones a enteros completadas.\")\n",
    "\n",
    "# --- FORMATO FINAL DE unplug_hourTime a 'YYYY-MM-DDTHH:MM:SS' como STRING ---\n",
    "# Este es el último paso, después del ordenamiento (que ahora no existe)\n",
    "if 'unplug_hourTime' in consolidated_df.columns and pd.api.types.is_datetime64_any_dtype(consolidated_df['unplug_hourTime']):\n",
    "    print(\"\\n--- Formateando 'unplug_hourTime' a 'YYYY-MM-DDTHH:MM:SS' (string) ---\")\n",
    "    consolidated_df['unplug_hourTime'] = consolidated_df['unplug_hourTime'].dt.strftime('%Y-%m-%dT%H:%M:%S').replace({pd.NaT: pd.NA})\n",
    "    print(\"Formato final aplicado a 'unplug_hourTime'.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- FIN DEL PROCESAMIENTO GENERAL ---\")\n",
    "\n",
    "# --- Mostrar el resultado final ---\n",
    "if 'consolidated_df' in locals() and not consolidated_df.empty:\n",
    "    print(\"\\n--- RESULTADO FINAL: DataFrame Consolidado ---\")\n",
    "    print(\"Primeras 5 filas:\") # Ya no se asume que está ordenado aquí\n",
    "    print(consolidated_df.head())\n",
    "    print(f\"\\nDimensiones del DataFrame consolidado: {consolidated_df.shape}\")\n",
    "    print(\"\\nTipos de datos del DataFrame consolidado (después de conversiones finales):\")\n",
    "    print(consolidated_df.dtypes) \n",
    "    print(\"\\nColumnas del DataFrame consolidado:\")\n",
    "    print(consolidated_df.columns.tolist())\n",
    "    \n",
    "    if 'unplug_hourTime' in consolidated_df.columns: \n",
    "        print(f\"\\nLa columna 'unplug_hourTime' ahora es de tipo: {consolidated_df['unplug_hourTime'].dtype}\")\n",
    "    \n",
    "    if 'unplug_hourTime' in consolidated_df.columns:\n",
    "        num_na_final = consolidated_df['unplug_hourTime'].isna().sum()\n",
    "        if num_na_final > 0:\n",
    "            print(f\"\\nNúmero de filas con 'unplug_hourTime' nulo/inválido (NaN/pd.NA) en el DataFrame final: {num_na_final}\")\n",
    "        else:\n",
    "            print(f\"\\n¡Excelente! No se encontraron valores nulos/inválidos en 'unplug_hourTime' en el DataFrame final.\")\n",
    "else:\n",
    "    print(\"\\nEl DataFrame consolidado final está vacío. Verifica la ruta y los archivos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3a144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Número total de filas en el DataFrame consolidado: 3678086\n"
     ]
    }
   ],
   "source": [
    "# contar numero de filas del df\n",
    "print(f\"\\nNúmero total de filas en el DataFrame consolidado: {len(consolidated_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30b6ba6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unplug_hourTime</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>idunplug_station</th>\n",
       "      <th>idplug_station</th>\n",
       "      <th>idplug_base</th>\n",
       "      <th>idunplug_base</th>\n",
       "      <th>unlock_station_name</th>\n",
       "      <th>lock_station_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>284</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>666</td>\n",
       "      <td>24</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>662</td>\n",
       "      <td>24</td>\n",
       "      <td>117</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>708</td>\n",
       "      <td>82</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "      <td>171</td>\n",
       "      <td>169</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unplug_hourTime  travel_time  idunplug_station  idplug_station  \\\n",
       "0  2018-01-01T00:00:00          284                 6               7   \n",
       "1  2018-01-01T00:00:00          666                24             117   \n",
       "2  2018-01-01T00:00:00          662                24             117   \n",
       "3  2018-01-01T00:00:00          708                82             110   \n",
       "4  2018-01-01T00:00:00          171               169              58   \n",
       "\n",
       "   idplug_base  idunplug_base unlock_station_name lock_station_name  \n",
       "0            1             14                 NaN               NaN  \n",
       "1            4             21                 NaN               NaN  \n",
       "2           19             19                 NaN               NaN  \n",
       "3            1              2                 NaN               NaN  \n",
       "4            5              3                 NaN               NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver las primeras filas para validar\n",
    "consolidated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36b5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar el DataFrame como un archivo CSV\n",
    "consolidated_df.to_csv('bdcompletabicimadcsvprueba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ec98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista con el orden deseado de las columnas\n",
    "columnas_orden_deseado = ['idunplug_station', 'idplug_station', 'travel_time', 'idplug_base',\n",
    "                            'idunplug_base','unplug_hourTime']\n",
    "consolidated_df = consolidated_df[columnas_orden_deseado]\n",
    "\n",
    "columnas_eliminar = [\"unlock_station_name\", \"lock_station_name\"]\n",
    "consolidated_df.drop(columns=columnas_eliminar, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae01edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idunplug_station</th>\n",
       "      <th>idplug_station</th>\n",
       "      <th>travel_time</th>\n",
       "      <th>idplug_base</th>\n",
       "      <th>idunplug_base</th>\n",
       "      <th>unplug_hourTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>284</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>117</td>\n",
       "      <td>666</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>117</td>\n",
       "      <td>662</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>110</td>\n",
       "      <td>708</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169</td>\n",
       "      <td>58</td>\n",
       "      <td>171</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-01T00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idunplug_station  idplug_station  travel_time  idplug_base  idunplug_base  \\\n",
       "0                 6               7          284            1             14   \n",
       "1                24             117          666            4             21   \n",
       "2                24             117          662           19             19   \n",
       "3                82             110          708            1              2   \n",
       "4               169              58          171            5              3   \n",
       "\n",
       "       unplug_hourTime  \n",
       "0  2018-01-01T00:00:00  \n",
       "1  2018-01-01T00:00:00  \n",
       "2  2018-01-01T00:00:00  \n",
       "3  2018-01-01T00:00:00  \n",
       "4  2018-01-01T00:00:00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Optimizado: convertir varias columnas a entero de forma segura\n",
    "cols_to_int = ['idplug_base','idplug_station']\n",
    "consolidated_df[cols_to_int] = consolidated_df[cols_to_int].apply(pd.to_numeric, errors='coerce').astype('Int64')\n",
    "consolidated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817e605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idunplug_station    0\n",
      "idplug_station      0\n",
      "travel_time         0\n",
      "idplug_base         0\n",
      "idunplug_base       0\n",
      "unplug_hourTime     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# contar nulos de todas las columnas\n",
    "nulos_por_columna_csv = consolidated_df.isnull().sum()\n",
    "print(nulos_por_columna_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdfed381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar el DataFrame como un archivo CSV\n",
    "consolidated_df.to_csv('bdcompleta_usos_bicimad_2018.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
