{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48b4d27",
   "metadata": {},
   "source": [
    "Utilizamos este modelo inicialmente pero por tener una gran cantidad de datos tuvimos que seleccionar un nueo modelo ya que siempre se queda por capacidad, haciendo que se repitiera el proceso varias veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f02b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>cant_unplug_hora</th>\n",
       "      <th>cant_plug_hora</th>\n",
       "      <th>dia</th>\n",
       "      <th>mes</th>\n",
       "      <th>año</th>\n",
       "      <th>dia_semana</th>\n",
       "      <th>hour</th>\n",
       "      <th>Valor_Temp</th>\n",
       "      <th>Valor_Prec</th>\n",
       "      <th>dia_especial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id  cant_unplug_hora  cant_plug_hora  dia  mes   año  dia_semana  \\\n",
       "0           1                 0               1    1    1  2018           0   \n",
       "1           1                 0               1    1    1  2018           0   \n",
       "2           1                 2               0    1    1  2018           0   \n",
       "3           1                 0               1    1    1  2018           0   \n",
       "4           1                 0               8    1    1  2018           0   \n",
       "\n",
       "   hour  Valor_Temp  Valor_Prec  dia_especial  \n",
       "0     4         6.0         0.0             1  \n",
       "1     7         5.0         0.0             1  \n",
       "2     8         5.0         0.0             1  \n",
       "3     9         5.0         0.0             1  \n",
       "4    10         5.0         0.0             1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "basedatos_df = pd.read_csv('basedatos_modelo_final.csv', sep =';',encoding='utf-8')\n",
    "basedatos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1281724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Target Encoder para 'cant_plug_hora'...\n",
      "Encoder para 'cant_plug_hora' guardado como 'encoder_plug.pkl'.\n",
      "\n",
      "Entrenando Target Encoder para 'cant_unplug_hora'...\n",
      "Encoder para 'cant_unplug_hora' guardado como 'encoder_unplug.pkl'.\n",
      "\n",
      "Aplicando Target Encoding a los DataFrames para el entrenamiento...\n",
      "Target Encoding aplicado. DataFrames 'df_for_model_plug' y 'df_for_model_unplug' listos.\n",
      "\n",
      "Características que se usarán en el modelo: ['station_id', 'dia', 'mes', 'hour', 'año', 'dia_semana', 'Valor_Temp', 'Valor_Prec', 'dia_especial']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from category_encoders import TargetEncoder\n",
    "import joblib # Para guardar los encoders\n",
    "\n",
    "# --- Definir las variables objetivo y las características categóricas ---\n",
    "target_plug = 'cant_plug_hora'\n",
    "target_unplug = 'cant_unplug_hora'\n",
    "\n",
    "# Las columnas categóricas a las que aplicaremos Target Encoding\n",
    "categorical_cols = ['station_id', 'dia', 'mes', 'hour', 'año', 'dia_semana']\n",
    "\n",
    "# Features numéricas/binarias (no se les hace Target Encoding)\n",
    "numerical_or_binary_features = ['Valor_Temp', 'Valor_Prec', 'dia_especial']\n",
    "\n",
    "# --- Crear y entrenar TargetEncoder para 'cant_plug_hora' ---\n",
    "print(\"Entrenando Target Encoder para 'cant_plug_hora'...\")\n",
    "encoder_plug = TargetEncoder(cols=categorical_cols)\n",
    "encoder_plug.fit(basedatos_df[categorical_cols], basedatos_df[target_plug])\n",
    "joblib.dump(encoder_plug, 'encoder_plug.pkl')\n",
    "print(\"Encoder para 'cant_plug_hora' guardado como 'encoder_plug.pkl'.\")\n",
    "\n",
    "\n",
    "# --- Crear y entrenar TargetEncoder para 'cant_unplug_hora' ---\n",
    "print(\"\\nEntrenando Target Encoder para 'cant_unplug_hora'...\")\n",
    "encoder_unplug = TargetEncoder(cols=categorical_cols)\n",
    "encoder_unplug.fit(basedatos_df[categorical_cols], basedatos_df[target_unplug])\n",
    "joblib.dump(encoder_unplug, 'encoder_unplug.pkl')\n",
    "print(\"Encoder para 'cant_unplug_hora' guardado como 'encoder_unplug.pkl'.\")\n",
    "\n",
    "\n",
    "# --- Aplicar los Target Encodings a una copia de los DataFrames para los modelos ---\n",
    "# Esto creará dos DataFrames separados, cada uno con sus categóricas Target Encoded\n",
    "# usando el encoder correspondiente a su target.\n",
    "df_for_model_plug = basedatos_df.copy()\n",
    "df_for_model_unplug = basedatos_df.copy()\n",
    "\n",
    "print(\"\\nAplicando Target Encoding a los DataFrames para el entrenamiento...\")\n",
    "df_for_model_plug[categorical_cols] = encoder_plug.transform(df_for_model_plug[categorical_cols])\n",
    "df_for_model_unplug[categorical_cols] = encoder_unplug.transform(df_for_model_unplug[categorical_cols])\n",
    "\n",
    "print(\"Target Encoding aplicado. DataFrames 'df_for_model_plug' y 'df_for_model_unplug' listos.\")\n",
    "\n",
    "# Definir la lista de todas las características para los modelos\n",
    "# (categóricas ya encodificadas + numéricas/binarias)\n",
    "base_features_independent = categorical_cols + numerical_or_binary_features\n",
    "\n",
    "print(f\"\\nCaracterísticas que se usarán en el modelo: {base_features_independent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614cffb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- División de datos para el Modelo de Enganches (cant_plug_hora) ---\n",
      "Dimensiones X1_train (plug): (5585712, 9)\n",
      "Dimensiones X1_test (plug): (1396429, 9)\n",
      "Dimensiones y1_train (plug): (5585712,)\n",
      "Dimensiones y1_test (plug): (1396429,)\n",
      "\n",
      "--- División de datos para el Modelo de Desenganches (cant_unplug_hora) ---\n",
      "Dimensiones X2_train (unplug): (5585712, 9)\n",
      "Dimensiones X2_test (unplug): (1396429, 9)\n",
      "Dimensiones y2_train (unplug): (5585712,)\n",
      "Dimensiones y2_test (unplug): (1396429,)\n",
      "\n",
      "Datos divididos en conjuntos de entrenamiento y prueba.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd # Asegúrate de tener pandas importado aquí si ejecutas solo este bloque\n",
    "\n",
    "# Las variables 'df_for_model_plug', 'df_for_model_unplug', 'target_plug', 'target_unplug',\n",
    "# y 'base_features_independent' deben estar definidas desde el Paso 1.\n",
    "\n",
    "print(\"\\n--- División de datos para el Modelo de Enganches (cant_plug_hora) ---\")\n",
    "X1_features_for_model = base_features_independent\n",
    "X1 = df_for_model_plug[X1_features_for_model]\n",
    "y1 = df_for_model_plug[target_plug]\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dimensiones X1_train (plug): {X1_train.shape}\")\n",
    "print(f\"Dimensiones X1_test (plug): {X1_test.shape}\")\n",
    "print(f\"Dimensiones y1_train (plug): {y1_train.shape}\")\n",
    "print(f\"Dimensiones y1_test (plug): {y1_test.shape}\")\n",
    "\n",
    "print(\"\\n--- División de datos para el Modelo de Desenganches (cant_unplug_hora) ---\")\n",
    "X2_features_for_model = base_features_independent\n",
    "X2 = df_for_model_unplug[X2_features_for_model]\n",
    "y2 = df_for_model_unplug[target_unplug]\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dimensiones X2_train (unplug): {X2_train.shape}\")\n",
    "print(f\"Dimensiones X2_test (unplug): {X2_test.shape}\")\n",
    "print(f\"Dimensiones y2_train (unplug): {y2_train.shape}\")\n",
    "print(f\"Dimensiones y2_test (unplug): {y2_test.shape}\")\n",
    "\n",
    "print(\"\\nDatos divididos en conjuntos de entrenamiento y prueba.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8172668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Entrenamiento y Evaluación para el MODELO PLUG (cant_plug_hora) ---\n",
      "Entrenando Random Forest para cant_plug_hora...\n",
      "\n",
      "Métricas Random Forest para cant_plug_hora:\n",
      "  MAE: 1.43\n",
      "  MSE: 4.18\n",
      "  RMSE: 2.04\n",
      "  R²: 0.47\n",
      "Modelo 'model_plug_rf.pkl' guardado.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor # Importa Random Forest\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd # Asegúrate de tener pandas importado aquí si ejecutas solo este bloque\n",
    "\n",
    "\n",
    "# Las variables X1_train, X1_test, y1_train, y1_test, X2_train, X2_test, y2_train, y2_test\n",
    "# deben estar definidas desde el Paso 2.\n",
    "\n",
    "print(\"\\n--- Entrenamiento y Evaluación para el MODELO PLUG (cant_plug_hora) ---\")\n",
    "\n",
    "# --- Modelo Random Forest para Enganches ---\n",
    "# Se recomienda un n_estimators más bajo que LightGBM para empezar,\n",
    "# ya que Random Forest es más intensivo computacionalmente por árbol.\n",
    "# Puedes ajustar n_estimators y otros parámetros como max_depth, min_samples_leaf.\n",
    "model_plug_rf = RandomForestRegressor(n_estimators=30, random_state=42, n_jobs=-1) # n_jobs=-1 usa todos los cores\n",
    "print(\"Entrenando Random Forest para cant_plug_hora...\")\n",
    "model_plug_rf.fit(X1_train, y1_train)\n",
    "\n",
    "predictions_plug_rf = model_plug_rf.predict(X1_test)\n",
    "mae_plug_rf = mean_absolute_error(y1_test, predictions_plug_rf)\n",
    "mse_plug_rf = mean_squared_error(y1_test, predictions_plug_rf)\n",
    "rmse_plug_rf = np.sqrt(mse_plug_rf)\n",
    "r2_plug_rf = r2_score(y1_test, predictions_plug_rf)\n",
    "\n",
    "print(f'\\nMétricas Random Forest para cant_plug_hora:')\n",
    "print(f'  MAE: {mae_plug_rf:.2f}')\n",
    "print(f'  MSE: {mse_plug_rf:.2f}')\n",
    "print(f'  RMSE: {rmse_plug_rf:.2f}')\n",
    "print(f'  R²: {r2_plug_rf:.2f}')\n",
    "#joblib.dump(model_plug_rf, 'model_plug_rf.pkl')\n",
    "print(\"Modelo 'model_plug_rf.pkl' guardado.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a22ebcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Entrenamiento y Evaluación para el MODELO UNPLUG (cant_unplug_hora) ---\n",
      "Entrenando Random Forest para cant_unplug_hora...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 536870912 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n    return self.func(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 189, in _parallel_build_trees\n    tree._fit(\n  File \"c:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n  File \"_tree.pyx\", line 153, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"_tree.pyx\", line 268, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n  File \"_tree.pyx\", line 923, in sklearn.tree._tree.Tree._add_node\n  File \"_tree.pyx\", line 891, in sklearn.tree._tree.Tree._resize_c\n  File \"_utils.pyx\", line 29, in sklearn.tree._utils.safe_realloc\nMemoryError: could not allocate 536870912 bytes\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m model_unplug_rf = RandomForestRegressor(n_estimators=\u001b[32m30\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEntrenando Random Forest para cant_unplug_hora...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel_unplug_rf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX2_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m predictions_unplug_rf = model_unplug_rf.predict(X2_test)\n\u001b[32m      9\u001b[39m mae_unplug_rf = mean_absolute_error(y2_test, predictions_unplug_rf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1748\u001b[39m \n\u001b[32m   1749\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1751\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1752\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1755\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    739\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    742\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    743\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    744\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ProjectSupport\\Documents\\David\\Cursos\\Master Data Science\\TFM\\Entrega TFM\\.venv\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: could not allocate 536870912 bytes"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Entrenamiento y Evaluación para el MODELO UNPLUG (cant_unplug_hora) ---\")\n",
    "\n",
    "# --- Modelo Random Forest para Desenganches ---\n",
    "model_unplug_rf = RandomForestRegressor(n_estimators=30, random_state=42, n_jobs=-1)\n",
    "print(\"Entrenando Random Forest para cant_unplug_hora...\")\n",
    "model_unplug_rf.fit(X2_train, y2_train)\n",
    "\n",
    "predictions_unplug_rf = model_unplug_rf.predict(X2_test)\n",
    "mae_unplug_rf = mean_absolute_error(y2_test, predictions_unplug_rf)\n",
    "mse_unplug_rf = mean_squared_error(y2_test, predictions_unplug_rf)\n",
    "rmse_unplug_rf = np.sqrt(mse_unplug_rf)\n",
    "r2_unplug_rf = r2_score(y2_test, predictions_unplug_rf)\n",
    "\n",
    "print(f'\\nMétricas Random Forest para cant_unplug_hora:')\n",
    "print(f'  MAE: {mae_unplug_rf:.2f}')\n",
    "print(f'  MSE: {mse_unplug_rf:.2f}')\n",
    "print(f'  RMSE: {rmse_unplug_rf:.2f}')\n",
    "print(f'  R²: {r2_unplug_rf:.2f}')\n",
    "#joblib.dump(model_unplug_rf, 'model_unplug_rf.pkl')\n",
    "print(\"Modelo 'model_unplug_rf.pkl' guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d69cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd # Asegúrate de importar pandas\n",
    "\n",
    "# Ejemplo de muestreo del 10%\n",
    "# Si tus datos son realmente muy grandes, puedes empezar con 0.01 (1%) o 0.05 (5%)\n",
    "sample_fraction = 0.05\n",
    "subset_size = int(X1_train.shape[0] * sample_fraction)\n",
    "\n",
    "# Genera índices aleatorios para el subset\n",
    "np.random.seed(42) # Para reproducibilidad\n",
    "random_indices = np.random.choice(X1_train.index, size=subset_size, replace=False)\n",
    "\n",
    "X_train_subset = X1_train.loc[random_indices]\n",
    "y_train_subset = y1_train.loc[random_indices]\n",
    "\n",
    "print(f\"Tamaño del subset de entrenamiento para tuning: {X_train_subset.shape}\")\n",
    "print(f\"Tamaño de las etiquetas del subset: {y_train_subset.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Reducir la cuadrícula de hiperparámetros (param_grid) ---\n",
    "# Usa rangos más pequeños y menos opciones, especialmente para n_estimators.\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 60],  # ¡Valores MUCHO más pequeños para empezar!\n",
    "    'max_depth': [5, 10, 15],     # Profundidades más limitadas\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "\n",
    "# --- 3. Inicializar el modelo Random Forest Regressor ---\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# --- 4. Inicializar GridSearchCV ---\n",
    "# n_jobs=-1 usa todos los núcleos de tu CPU para acelerar (si tienes memoria suficiente por núcleo)\n",
    "# cv=3 usa 3-fold cross-validation, reduce el tiempo de evaluación\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# --- 5. Entrenar GridSearchCV en el SUBSET ---\n",
    "print(\"\\nIniciando GridSearchCV en el subset de datos...\")\n",
    "grid_search.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# --- 6. Mostrar los mejores parámetros y el mejor score ---\n",
    "print(\"\\nMejores parámetros encontrados:\", grid_search.best_params_)\n",
    "print(\"Mejor score (MSE negativo):\", grid_search.best_score_)\n",
    "\n",
    "# El mejor modelo entrenado lo obtienes así:\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "print(\"\\nMejor modelo obtenido de GridSearchCV:\", best_rf_model)\n",
    "\n",
    "\n",
    "# Opcional: Entrenar el mejor modelo en el *conjunto de entrenamiento COMPLETO*\n",
    "# print(\"\\nEntrenando el mejor modelo con el conjunto de entrenamiento COMPLETO...\")\n",
    "# best_rf_model_full = RandomForestRegressor(**grid_search.best_params_, random_state=42)\n",
    "# best_rf_model_full.fit(X1_train_encoded, y1_train)\n",
    "# print(\"Modelo entrenado en conjunto completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del subset de entrenamiento para tuning: (232650, 8)\n",
      "Tamaño de las etiquetas del subset: (232650,)\n",
      "\n",
      "Iniciando GridSearchCV en el subset de datos...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "Mejores parámetros encontrados: {'max_depth': 15, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 60}\n",
      "Mejor score (MSE negativo): -4.511792169993154\n",
      "\n",
      "Mejor modelo obtenido de GridSearchCV: RandomForestRegressor(max_depth=15, min_samples_leaf=3, n_estimators=60,\n",
      "                      random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd # Asegúrate de importar pandas\n",
    "\n",
    "# Ejemplo de muestreo del 10%\n",
    "# Si tus datos son realmente muy grandes, puedes empezar con 0.01 (1%) o 0.05 (5%)\n",
    "sample_fraction = 0.05\n",
    "subset_size = int(X1_train.shape[0] * sample_fraction)\n",
    "\n",
    "# Genera índices aleatorios para el subset\n",
    "np.random.seed(42) # Para reproducibilidad\n",
    "random_indices = np.random.choice(X1_train.index, size=subset_size, replace=False)\n",
    "\n",
    "X_train_subset = X1_train.loc[random_indices]\n",
    "y_train_subset = y1_train.loc[random_indices]\n",
    "\n",
    "print(f\"Tamaño del subset de entrenamiento para tuning: {X_train_subset.shape}\")\n",
    "print(f\"Tamaño de las etiquetas del subset: {y_train_subset.shape}\")\n",
    "\n",
    "\n",
    "# --- 2. Reducir la cuadrícula de hiperparámetros (param_grid) ---\n",
    "# Usa rangos más pequeños y menos opciones, especialmente para n_estimators.\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 60],  # ¡Valores MUCHO más pequeños para empezar!\n",
    "    'max_depth': [5, 10, 15],     # Profundidades más limitadas\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 3]\n",
    "}\n",
    "\n",
    "\n",
    "# --- 3. Inicializar el modelo Random Forest Regressor ---\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# --- 4. Inicializar GridSearchCV ---\n",
    "# n_jobs=-1 usa todos los núcleos de tu CPU para acelerar (si tienes memoria suficiente por núcleo)\n",
    "# cv=3 usa 3-fold cross-validation, reduce el tiempo de evaluación\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# --- 5. Entrenar GridSearchCV en el SUBSET ---\n",
    "print(\"\\nIniciando GridSearchCV en el subset de datos...\")\n",
    "grid_search.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# --- 6. Mostrar los mejores parámetros y el mejor score ---\n",
    "print(\"\\nMejores parámetros encontrados:\", grid_search.best_params_)\n",
    "print(\"Mejor score (MSE negativo):\", grid_search.best_score_)\n",
    "\n",
    "# El mejor modelo entrenado lo obtienes así:\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "print(\"\\nMejor modelo obtenido de GridSearchCV:\", best_rf_model)\n",
    "\n",
    "\n",
    "# Opcional: Entrenar el mejor modelo en el *conjunto de entrenamiento COMPLETO*\n",
    "# print(\"\\nEntrenando el mejor modelo con el conjunto de entrenamiento COMPLETO...\")\n",
    "# best_rf_model_full = RandomForestRegressor(**grid_search.best_params_, random_state=42)\n",
    "# best_rf_model_full.fit(X1_train_encoded, y1_train)\n",
    "# print(\"Modelo entrenado en conjunto completo.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
